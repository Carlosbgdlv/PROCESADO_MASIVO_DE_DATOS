{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPOCESADO PYSPARK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Barquero Gómez de la Venta, Carlos\n",
    "\n",
    "González Carrasco, Paula\n",
    "\n",
    "Pérez Alba, Marina\n",
    "\n",
    "Pérez Ortiz, Gema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WXRAZVTbKVwD",
    "outputId": "51cf63e7-8bf5-4627-ac46-4b6d5ab162a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/b0/9d6860891ab14a39d4bddf80ba26ce51c2f9dc4805e5c6978ac0472c120a/pyspark-3.1.1.tar.gz (212.3MB)\n",
      "\u001b[K     |████████████████████████████████| 212.3MB 62kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 16.4MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767604 sha256=ec462554c811035a544e0ff4b30c27f33e67d2b19232d816c7595eab449b1f56\n",
      "  Stored in directory: /root/.cache/pip/wheels/0b/90/c0/01de724414ef122bd05f056541fb6a0ecf47c7ca655f8b3c0f\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9 pyspark-3.1.1\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1oKckeQ7KRtp"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "try:\n",
    "    sc = SparkContext('local', 'Pyspark demo')\n",
    "except ValueError:\n",
    "    print('SparkContext already exists!')\n",
    "from pyspark.sql import SparkSession\n",
    "try:\n",
    "    spark = SparkSession.builder.appName('Recommendation_system').getOrCreate()\n",
    "except ValueError:\n",
    "    print('SparkSession already exists!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7DOVOtwLL6r",
    "outputId": "37aab6d1-0e70-4ad9-a682-a4143fced5fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TI7fV0DcBA7Q"
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HOhzoXhkGv7x"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "import datetime\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "from pyspark.sql.types import StringType,BooleanType,DateType, DoubleType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YLtnrltzkuJn"
   },
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8Go8eJpZVGvF"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,types\n",
    "from pyspark.sql.functions import unix_timestamp, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import TimestampType, StructType\n",
    "from operator import attrgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "seQxS-F5G4y9",
    "outputId": "dd1e0c3d-9eaa-4f47-d8f6-737f777ab637"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.16568899154663\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "spark_df1 = spark.read.csv(\"/content/drive/MyDrive/country_vaccinations.csv\", header = True)\n",
    "spark_df2 = spark.read.csv(\"/content/drive/MyDrive/covid19_country_population.csv\", header = True)\n",
    "\n",
    "spark_df2 = spark_df2.withColumnRenamed(\"CountryAlpha3Code\", \"iso_code\")\n",
    "\n",
    "spark_df1 = spark_df1.drop(spark_df1.daily_vaccinations_raw)\n",
    "\n",
    "spark_df1=spark_df1.withColumn(\"date\",unix_timestamp(\"date\", 'yyyy-MM-dd').cast(TimestampType()))\n",
    "\n",
    "spark_df1 = spark_df1.withColumn(\"total_vaccinations\",spark_df1.total_vaccinations.cast(DoubleType()))\n",
    "spark_df1 = spark_df1.withColumn(\"people_vaccinated\",spark_df1.total_vaccinations.cast(DoubleType()))\n",
    "spark_df1 = spark_df1.withColumn(\"people_fully_vaccinated\",spark_df1.total_vaccinations.cast(DoubleType()))\n",
    "spark_df1 = spark_df1.withColumn(\"daily_vaccinations\",spark_df1.total_vaccinations.cast(DoubleType()))\n",
    "spark_df1 = spark_df1.withColumn(\"total_vaccinations_per_hundred\",spark_df1.total_vaccinations.cast(DoubleType()))\n",
    "spark_df1 = spark_df1.withColumn(\"people_vaccinated_per_hundred\",spark_df1.total_vaccinations.cast(DoubleType()))\n",
    "spark_df1 = spark_df1.withColumn(\"people_fully_vaccinated_per_hundred\",spark_df1.total_vaccinations.cast(DoubleType()))\n",
    "spark_df1 = spark_df1.withColumn(\"daily_vaccinations_per_million\",spark_df1.total_vaccinations.cast(DoubleType()))\n",
    "\n",
    "newDF = spark_df1.join(spark_df2, spark_df1.iso_code == spark_df2.iso_code, 'inner').drop(spark_df2.iso_code)\n",
    "\n",
    "@pandas_udf(newDF.schema, PandasUDFType.GROUPED_MAP)\n",
    "def interpolate(pdf):\n",
    "    pdf = pdf.set_index('date')\n",
    "    #pdf.sort_values(by=['a'], inplace=True)\n",
    "    pdf = pdf.interpolate(method='cubicspline', axis=0, limit_direction='forward')\n",
    "    pdf.reset_index(inplace=True)\n",
    "    return pdf\n",
    "\n",
    "newDF = newDF.groupby(['country']).apply(interpolate)\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Preprocesado pyspark.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
